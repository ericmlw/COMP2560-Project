{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import sys\n",
    "sys.path.append(\"FederatedLearning-main\")\n",
    "import Model\n",
    "\n",
    "scenario_one=pd.read_csv(\"database/Scenario_1/Scenario_1_C.csv\")\n",
    "scenario_two=pd.read_csv(\"database/Scenario_2/Scenario_2_C.csv\")\n",
    "scenario_three=pd.read_csv(\"database/Scenario_3/Scenario_3_C.csv\")\n",
    "scenarios=[scenario_one,scenario_two,scenario_three]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NaN elimination and standard scaling\n",
    "def columnwise_na_elimiation(data):\n",
    "    \"\"\"\n",
    "    The method filter out all columns contains NA value in the set\n",
    "\n",
    "    Input: pandas dataframe\n",
    "    Output: non-NA containing pandas dataframe \n",
    "    \"\"\"\n",
    "    col_na=data.isna().sum(axis=0)\n",
    "    # print(col_na)\n",
    "    col_na[col_na!=0]=1\n",
    "    col_na=col_na.astype(bool)\n",
    "    # print(col_na)\n",
    "    col_na=~col_na\n",
    "    # print(col_na)\n",
    "    filtered_data=data.loc[:,col_na]\n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "#naelimination and splitting\n",
    "nanFreeSet=[]\n",
    "for scenario in scenarios:\n",
    "    nanfreedata=columnwise_na_elimiation(scenario)\n",
    "    nanFreeSet.append(nanfreedata)\n",
    "\n",
    "train_set=[]\n",
    "testing_set=[]\n",
    "\n",
    "for nanfreeData in nanFreeSet:\n",
    "    train, test = train_test_split(nanfreeData, test_size=0.2)\n",
    "    trainX = train.iloc[:, :-2]\n",
    "    trainY = train.iloc[:, -2:]\n",
    "    testX = test.iloc[:, :-2]\n",
    "    testY = test.iloc[:, -2:]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    trainX_scaled = scaler.fit_transform(trainX)\n",
    "    testX_scaled = scaler.transform(testX)\n",
    "\n",
    "    pca = PCA(n_components=5)\n",
    "    trainX_pca = pca.fit_transform(trainX_scaled)\n",
    "    testX_pca = pca.transform(testX_scaled)\n",
    "\n",
    "    train_set.append((trainX_pca,trainY))\n",
    "    testing_set.append((testX_pca,testY))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training\n",
    "def training(model,input_lr,trainX,trainY):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=input_lr)\n",
    "    mse_loss = Loss.MSELoss()\n",
    "\n",
    "    # Convert data to tensors\n",
    "    trainX_tensor = torch.tensor(trainX, dtype=torch.float32)\n",
    "    trainY_tensor = torch.tensor(np.array(trainY), dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(1000):\n",
    "        optimizer.zero_grad()\n",
    "        predicted_result = model(trainX_tensor)\n",
    "        loss = mse_loss(predicted_result, trainY_tensor,model)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    #     if (epoch>990):\n",
    "    #         print(f\"Epoch [{epoch + 1}/10], Loss: {loss.item()}\")\n",
    "    # print(list(model.parameters()))\n",
    "\n",
    "def scoring(model,testX,testY):\n",
    "    testX=torch.tensor(testX,dtype=torch.float32)\n",
    "    return ((mean_squared_error(testY,(model(testX)).detach().numpy())),r2_score(testY,(model(testX)).detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fermi_energy for scenario  1  has MSE of:  1.251528492758159  r2  of  -0.45685475073674464\n",
      "thermo_prob_norm for scenario  1  has MSE of:  0.010605425000298385  r2  of  0.0918002878642219\n",
      "Fermi_energy for scenario  2  has MSE of:  1.5215408891895215  r2  of  -0.23529014324160036\n",
      "thermo_prob_norm for scenario  2  has MSE of:  0.040712470774379235  r2  of  0.06990209796458113\n",
      "Fermi_energy for scenario  3  has MSE of:  1.2654416032419364  r2  of  -0.4780324397467359\n",
      "thermo_prob_norm for scenario  3  has MSE of:  0.0028718277777898668  r2  of  0.7499324555885263\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    \n",
    "    trainX=train_set[i][0]\n",
    "    trainY=train_set[i][1].iloc[:,0]\n",
    "    testX=testing_set[i][0]\n",
    "    testY=testing_set[i][1].iloc[:,0]\n",
    "    model=Model.LinearRegression(trainX.shape[1],1)\n",
    "    training(model,0.001,trainX,trainY)\n",
    "    score=scoring(model,testX,testY)\n",
    "    print(\"Fermi_energy for scenario \",str(i+1),\" has MSE of: \",score[0],\" r2  of \",score[1])\n",
    "\n",
    "    trainY=train_set[i][1].iloc[:,1]\n",
    "    testY=testing_set[i][1].iloc[:,1]\n",
    "    model=Model.LinearRegression(trainX.shape[1],1)\n",
    "    training(model,0.001,trainX,trainY)\n",
    "    score=scoring(model,testX,testY)\n",
    "    print(\"thermo_prob_norm for scenario \",str(i+1),\" has MSE of: \",score[0],\" r2  of \",score[1])\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
